# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

openmatch:
  monitoring:
    jaeger:
      enabled: true
      agentEndpoint: "open-match-jaeger-agent:6831"
      collectorEndpoint: "http://open-match-jaeger-collector:14268/api/traces"
    prometheus:
      enabled: true
      endpoint: "/metrics"
      serviceDiscovery: true
    stackdriver:
      enabled: false
      gcpProjectId: "project_id"
      metricPrefix: "open_match"
    zipkin:
      enabled: false
      endpoint: "/zipkin"
      reporterEndpoint: "zipkin"
    reportingPeriod: "5s"
  backend:
    install: true
    grpc:
      port: 50505
    http:
      port: 51505
  frontend:
    install: true
    grpc:
      port: 50504
    http:
      port: 51504
  mmlogic:
    install: true
    grpc:
      port: 50503
    http:
      port: 51503
  evaluator:
    install: true
    grpc:
      port: 50506
    http:
      port: 51506
  functions:
    grpc:
      port: 50502
    http:
      port: 51502

  config:
    install: true
    mountPath: /config
    files:
      matchmaker_config.yaml:
        debug: true

        logging:
          level: debug
          format: text
          source: false
        # Open Match applies the exponential backoff strategy for its retryable gRPC calls.
        # The settings below are the default backoff configuration used in Open Match.
        # See https://github.com/cenkalti/backoff/blob/v3/exponential.go for detailed explanations
        backoff:
          # The initial retry interval (in milliseconds)
          initialInterval: 100ms
          # maxInterval caps the maximum time elapsed for a retry interval
          maxInterval: 500ms
          # The next retry interval is multiplied by this multiplier
          multiplier: 1.5
          # Randomize the retry interval
          randFactor: 0.5
          # maxElapsedTime caps the retry time (in milliseconds)
          maxElapsedTime: 3000ms

        api:
          backend:
            hostname: om-backend
            grpcport: "{{.Values.openmatch.backend.grpc.port}}"
            httpport: "{{.Values.openmatch.backend.http.port}}"
          frontend:
            hostname: om-frontend
            grpcport: "{{.Values.openmatch.frontend.grpc.port}}"
            httpport: "{{.Values.openmatch.frontend.http.port}}"
          mmlogic:
            hostname: om-mmlogic
            grpcport: "{{.Values.openmatch.mmlogic.grpc.port}}"
            httpport: "{{.Values.openmatch.mmlogic.http.port}}"
          evaluator:
            hostname: om-evaluator
            grpcport: "{{.Values.openmatch.evaluator.grpc.port}}"
            httpport: "{{.Values.openmatch.evaluator.http.port}}"
          functions:
            grpcport: "{{.Values.openmatch.functions.grpc.port}}"
            httpport: "{{.Values.openmatch.functions.http.port}}"

        evalutor:
          pollIntervalMs: 1000
          maxWaitMs: 10000

        monitoring:
          jaeger:
            enable: "{{.Values.openmatch.monitoring.jaeger.enabled}}"
            agentEndpoint: "{{.Values.openmatch.monitoring.jaeger.agentEndpoint}}"
            collectorEndpoint: "{{.Values.openmatch.monitoring.jaeger.collectorEndpoint}}"
          prometheus:
            enable: "{{.Values.openmatch.monitoring.prometheus.enabled}}"
            endpoint: "{{.Values.openmatch.monitoring.prometheus.endpoint}}"
          stackdriver:
            enable: "{{.Values.openmatch.monitoring.stackdriver.enabled}}"
            gcpProjectId: "{{.Values.openmatch.monitoring.stackdriver.gcpProjectId}}"
            metricPrefix: "{{.Values.openmatch.monitoring.stackdriver.metricPrefix}}"
          zipkin:
            enable: "{{.Values.openmatch.monitoring.zipkin.enabled}}"
            endpoint: "{{.Values.openmatch.monitoring.zipkin.endpoint}}"
            reporterEndpoint: "{{.Values.openmatch.monitoring.zipkin.reporterEndpoint}}"
          reportingPeriod: "{{.Values.openmatch.monitoring.reportingPeriod}}"

        queues:
          proposals:
            name: proposalq

        ignoreLists:
          proposed:
            name: proposed
            offset: 0
            duration: 800
          deindexed:
            name: deindexed
            offset: 0
            duration: 800
          expired:
            name: OM_METADATA.accessed
            offset: 800
            duration: 0
        storage:
          page:
            size: 10000
        redis:
          pool:
            maxIdle: 3
            maxActive: 0
            idleTimeout: 60
          queryArgs:
            count: 10000
          # TODO: remove unused config
          results:
            pageSize: 10000
          expiration: 43200

        jsonkeys:
          rosters: properties.rosters
          pools: properties.pools

        playerIndices:
        - char.cleric
        - char.knight
        - char.paladin
        - map.aleroth
        - map.oasis
        - mmr.rating
        - mode.battleroyale
        - mode.ctf
        - mode.demo
        - region.europe-east1
        - region.europe-west1
        - region.europe-west2
        - region.europe-west3
        - region.europe-west4
        - role.dps
        - role.support
        - role.tank

  image:
    registry: gcr.io/open-match-public-images
    tag: 0.0.0-dev
    backend:
      name: openmatch-backend
      pullPolicy: Always
    frontend:
      name: openmatch-frontend
      pullPolicy: Always
    mmlogic:
      name: openmatch-mmlogic
      pullPolicy: Always
    evaluator:
      name: openmatch-evaluator
      pullPolicy: Always

# https://hub.helm.sh/charts/stable/redis
# https://github.com/helm/charts/tree/master/stable/redis
redis:
  fullnameOverride: om-redis
  # TODO: Temporarily disable password protected Redis until all the examples are no longer talking directly to the database.
  usePassword: false
  master:
    disableCommands: [] # don't disable 'FLUSH-' commands
  metrics:
    enabled: true

# https://github.com/helm/charts/tree/master/stable/prometheus
prometheus:
  alertmanager:
    enabled: true
  nodeExporter:
    enabled: true
  kubeStateMetrics:
    enabled: true
  pushgateway:
    enabled: true
  server:
    resources:
      requests:
        memory: 4Gi
        cpu: 2
  prometheus.yml:
      rule_files:
        - /etc/config/rules
        - /etc/config/alerts

      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
              - localhost:9090

        # A scrape configuration for running Prometheus on a Kubernetes cluster.
        # This uses separate scrape configs for cluster components (i.e. API server, node)
        # and services to allow each to use different authentication configs.
        #
        # Kubernetes labels will be added as Prometheus labels on metrics via the
        # `labelmap` relabeling action.

        # Scrape config for API servers.
        #
        # Kubernetes exposes API servers as endpoints to the default/kubernetes
        # service so this uses `endpoints` role and uses relabelling to only keep
        # the endpoints associated with the default/kubernetes service using the
        # default named port `https`. This works for single API server deployments as
        # well as HA API server deployments.
        - job_name: 'kubernetes-apiservers'

          kubernetes_sd_configs:
            - role: endpoints

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https

        # Example scrape config for pods
        #
        # The relabeling allows the actual pod scrape endpoint to be configured via the
        # following annotations:
        #
        # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
        # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
        # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
        - job_name: 'kubernetes-pods'

          kubernetes_sd_configs:
            - role: pod

          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: kubernetes_namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name

grafana:
  persistence:
    enabled: true
  server:
    persistentVolume:
      size: 10Gi
  adminPassword: openmatch
  service:
    port: 3000
  sidecar:
      dashboards:
          enabled: true
  plugins: grafana-piechart-panel
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://open-match-prometheus-server.{{ .Release.Namespace }}.svc.cluster.local:80/
        access: proxy
        isDefault: true
